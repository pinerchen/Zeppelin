{"paragraphs":[{"text":"%dep\r\nz.load(\"org.scalanlp:jblas:1.2.1\")\r\n","user":"anonymous","dateUpdated":"2018-12-14T09:50:25+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@564ec774\n"}]},"apps":[],"jobName":"paragraph_1544775004200_1866385978","id":"20181212-015843_1688763497","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:12:51+0000","dateFinished":"2018-12-14T08:12:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:255"},{"text":"// 导入jblas库中的矩阵类\nimport org.jblas.DoubleMatrix\n// 定义相似度函数\ndef cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {\n    vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())\n}\n","user":"anonymous","dateUpdated":"2018-12-14T08:12:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.jblas.DoubleMatrix\ncosineSimilarity: (vec1: org.jblas.DoubleMatrix, vec2: org.jblas.DoubleMatrix)Double\n"}]},"apps":[],"jobName":"paragraph_1544775004205_1413848674","id":"20181212-015920_1065716449","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:12:51+0000","dateFinished":"2018-12-14T08:13:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"text":"import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n\nimport org.apache.spark.ml.fpm.FPGrowth\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.mllib.evaluation.RankingMetrics\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.explode\nimport org.apache.spark.sql.{DataFrame, Dataset,SparkSession}\n\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.sql.Row\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.rdd.RDD","user":"anonymous","dateUpdated":"2018-12-14T08:13:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport org.apache.spark.ml.fpm.FPGrowth\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.mllib.evaluation.RankingMetrics\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.explode\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.sql.Row\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.rdd.RDD\n"}]},"apps":[],"jobName":"paragraph_1544775004206_1542031858","id":"20181212-015934_389368932","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:13:16+0000","dateFinished":"2018-12-14T08:13:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"text":"//raw dataset\nval transaction = spark.read.option(\"header\",\"true\").csv(\"s3://input-smart-find/Quotations-110818-S.csv\").select($\"Quotation: Quotation ID\".alias(\"user\"), $\"Product: Product Family\".alias(\"product\"),$\"Display Quantity\".alias(\"number\")).groupBy(\"user\",\"product\").agg(sum(\"number\").alias(\"rating\")).toDF()\n val product_description = spark.read.option(\"header\",\"true\").csv(\"s3://input-smart-find/products - 110818-s.csv\").select($\"Product Family\".alias(\"product\"), $\"Product Description\".alias(\"product_description\"))\n\nval userIndexer = new StringIndexer().setInputCol(\"user\").setOutputCol(\"userID\")\nval indexed = userIndexer.fit(transaction).transform(transaction)\nval productIndexer = new StringIndexer().setInputCol(\"product\").setOutputCol(\"productID\")\nval whole = productIndexer.fit(indexed).transform(indexed)\n//whole.printSchema()\n//whole.show(10)\nval transform = whole.select($\"userID\".cast(\"int\"),$\"productID\".cast(\"int\"),$\"rating\")\nval table = transform.groupBy(\"userID\",\"productID\").agg(sum(\"rating\").alias(\"rating\")).toDF()","user":"anonymous","dateUpdated":"2018-12-14T08:13:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"transaction: org.apache.spark.sql.DataFrame = [user: string, product: string ... 1 more field]\nproduct_description: org.apache.spark.sql.DataFrame = [product: string, product_description: string]\nuserIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_49bc65f37cd0\nindexed: org.apache.spark.sql.DataFrame = [user: string, product: string ... 2 more fields]\nproductIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_55501f307794\nwhole: org.apache.spark.sql.DataFrame = [user: string, product: string ... 3 more fields]\ntransform: org.apache.spark.sql.DataFrame = [userID: int, productID: int ... 1 more field]\ntable: org.apache.spark.sql.DataFrame = [userID: int, productID: int ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=0","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=1","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=2","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=3"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544775004206_-1943191093","id":"20181212-020008_856232412","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:13:22+0000","dateFinished":"2018-12-14T08:14:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"text":"// table.count() //140062\nval numUsers = table.select(table.col(\"userID\")).distinct().count() \nval numProducts = table.select(table.col(\"productID\")).distinct().count()\n//numUsers //33984\n// numProducts //24209\n\n/*\n+------+---------+\n|unique|unique   |\n|user  |product  |\n+------+---------+\n| 33984|    24209|\n+------+---------+\n\n\n\n*/\n","user":"anonymous","dateUpdated":"2018-12-14T08:14:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"numUsers: Long = 33984\nnumProducts: Long = 24209\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=4","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=5"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544775004206_-1489511997","id":"20181212-023543_1975592944","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:14:35+0000","dateFinished":"2018-12-14T08:16:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"// filter某用戶在某產品購買數量最多的紀錄\n//whole.sort($\"rating\".desc).show \n/*\n+----------+----------+-------+-------+---------+\n|      user|   product| rating| userID|productID|\n+----------+----------+-------+-------+---------+\n|Q-00030883|10QYPAT1EU|30000.0|14060.0|     26.0|\n|Q-00063632|61B7JAT6EU|30000.0|17747.0|     60.0|\n|Q-00063632|61CAKAT1EU|30000.0|17747.0|    207.0|\n|Q-00030883|10M4S00R00|30000.0|14060.0|  10806.0|\n|Q-00068476|20LTS6P200|29200.0|10726.0|  23877.0|\n+----------+----------+-------+-------+---------+\n*/\n\n\n// filter 消費數量最高的前10大用戶\n//whole.groupBy(\"userID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc).show\n/*\n+------+------------+\n|userID|Total_Amount|\n+------+------------+\n|   3.0|    537000.0|\n|  54.0|    344450.0|\n| 282.0|    250800.0|\n| 334.0|    230800.0|\n|  71.0|    173000.0|\n| 719.0|    170800.0|\n|1054.0|    160000.0|\n| 685.0|    158000.0|\n| 707.0|    154000.0|\n| 760.0|    148000.0|\n+------+------------+\n\n*/\n\n// top 10 sale\nval top_sale =  whole.groupBy(\"productID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc)\n/*\n+---------+------------+\n|productID|Total_Amount|\n+---------+------------+\n|      0.0|    577598.0|\n|      1.0|    492920.0|\n|      5.0|    288709.0|\n|      4.0|    272528.0|\n|      3.0|    262920.0|\n|      8.0|    236287.0|\n|     18.0|    228088.0|\n|     11.0|    221678.0|\n|      2.0|    213747.0|\n|    153.0|    201092.0|\n+---------+------------+\n\n*/","user":"anonymous","dateUpdated":"2018-12-14T09:52:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"top_sale: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productID: double, Total_Amount: double]\n"}]},"apps":[],"jobName":"paragraph_1544775004207_-227861244","id":"20181212-020233_1752433362","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T09:52:51+0000","dateFinished":"2018-12-14T09:52:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260"},{"text":"// NOTE: add minimum and maximum values to thresholds\r\nval thresholds: Array[Double] = Array(Double.MinValue, 0.0) ++ (((0.0 until 600000 by 100000).toArray ++ Array(Double.MaxValue)).map(_.toDouble))\r\n\r\n// Convert DataFrame to RDD and calculate histogram values\r\nval _tmpHist = top_sale.\r\n    select($\"Total_Amount\" cast \"double\").\r\n    rdd.map(r => r.getDouble(0)).\r\n    histogram(thresholds)\r\n\r\n// Result DataFrame contains `from`, `to` range and the `value`.\r\nval histogram = sc.parallelize((thresholds, thresholds.tail, _tmpHist).zipped.toList).toDF(\"from\", \"to\", \"value\")","user":"anonymous","dateUpdated":"2018-12-14T09:57:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"thresholds: Array[Double] = Array(-1.7976931348623157E308, 0.0, 0.0, 100000.0, 200000.0, 300000.0, 400000.0, 500000.0, 1.7976931348623157E308)\n_tmpHist: Array[Long] = Array(0, 0, 24168, 31, 8, 0, 1, 1)\nhistogram: org.apache.spark.sql.DataFrame = [from: double, to: double ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=197","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=198"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544779918293_32042807","id":"20181214-093158_1815709665","dateCreated":"2018-12-14T09:31:58+0000","dateStarted":"2018-12-14T09:57:37+0000","dateFinished":"2018-12-14T09:59:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"val unique_product = table.dropDuplicates(\"productID\") //過濾單一產品\nval unique_user = table.dropDuplicates(\"userID\") //過濾單一用戶\nval join = unique_user.union(unique_product) //兩個資料表append\nval unique_product_user = join.dropDuplicates() //重複record drop掉 -> 作為train set\nval numUsers = unique_product_user.select(unique_product_user.col(\"userID\")).distinct().count() //確認數據有相同數量unique user\nval numProducts = unique_product_user.select(unique_product_user.col(\"productID\")).distinct().count() //確認數據有相同數量unique product\n//numUsers\n//numProducts","user":"anonymous","dateUpdated":"2018-12-14T08:16:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"unique_product: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\nunique_user: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\njoin: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\nunique_product_user: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\nnumUsers: Long = 33984\nnumProducts: Long = 24209\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=7","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=8"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544775004207_995560125","id":"20181213-021529_782513074","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:16:25+0000","dateFinished":"2018-12-14T08:18:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"val test_product = test.groupBy(\"productID\").agg(count(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc)\ntest_product.show","user":"anonymous","dateUpdated":"2018-12-14T10:28:16+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"test_product: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productID: int, Total_Amount: bigint]\n+---------+------------+\n|productID|Total_Amount|\n+---------+------------+\n|        0|        1999|\n|        1|        1276|\n|        2|         804|\n|        3|         676|\n|        4|         659|\n|        5|         644|\n|        6|         574|\n|       12|         494|\n|        7|         467|\n|        9|         452|\n|       10|         446|\n|        8|         433|\n|       13|         432|\n|       16|         409|\n|       14|         394|\n|       11|         391|\n|       15|         380|\n|       21|         371|\n|       23|         350|\n|       18|         345|\n+---------+------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=323","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=324"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544781852528_-1457830301","id":"20181214-100412_1126694641","dateCreated":"2018-12-14T10:04:12+0000","dateStarted":"2018-12-14T10:28:16+0000","dateFinished":"2018-12-14T10:29:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"text":"val test = table.except(unique_product_user)\nval train = unique_product_user\n//val numTrain = train.count()\n// val numTest = test.count()\n//test.select(unique_product_user.col(\"userID\")).distinct().count()\n//test.select(unique_product_user.col(\"productID\")).distinct().count()\n// train: 88943\n// test: 51057","user":"anonymous","dateUpdated":"2018-12-14T08:18:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1544775004207_-583498654","id":"20181213-030530_760267561","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:18:38+0000","dateFinished":"2018-12-14T08:18:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"text":"// val training = train.union(unique_product_tr.toDF())\n\nval trainRDD = train.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\n\nval testRDD = test.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\n","user":"anonymous","dateUpdated":"2018-12-14T08:18:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[156] at map at <console>:76\ntestRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[181] at map at <console>:75\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=9"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544775004208_2026659110","id":"20181212-043339_2062055690","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:18:39+0000","dateFinished":"2018-12-14T08:19:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"text":"/*\r\ndef computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], n: Long): Double = {\r\n    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))\r\n    val predictionsAndRatings = predictions.map(x => ((x.user, x.product), x.rating))\r\n      .join(data.map(x => ((x.user, x.product), x.rating)))\r\n      .values\r\n    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).reduce(_ + _) / n)\r\n  }","user":"anonymous","dateUpdated":"2018-12-14T08:19:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[]},"apps":[],"jobName":"paragraph_1544775004209_163378490","id":"20181212-052939_2138673281","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:19:30+0000","dateFinished":"2018-12-14T08:19:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"// Corss Validation Model\n    val ranks = List(20)\n    val lambdas = List(0.01)\n    val alphas = List(40)\n    val numIters = List(10, 15)\n    var bestModel: Option[MatrixFactorizationModel] = None\n    var bestRanking = 99.0\n    var bestRank = 0\n    var bestLambda = -1.0\n    var bestNumIter = -1\n    var bestAlpha = 0\n\n    for (rank <- ranks; lambda <- lambdas; numIter <- numIters; alpha <- alphas ) {\n      val model = ALS.trainImplicit(trainRDD, rank, numIter, lambda, alpha)\n      val rank_in = expectedPercentileRanking(model, testRDD)\n      println(\"Percentil Ranking = \" + rank_in + \" for the model trained with rank = \"\n        + rank + \n        \", lambda = \" + lambda + \n        \", alpha = \" + alpha + \n        \", and numIter = \" + numIter + \".\")\n      if (rank_in < bestRanking) {\n        bestModel = Some(model)\n        bestRanking = rank_in\n        bestRank = rank\n        bestLambda = lambda\n        bestNumIter = numIter\n        bestAlpha = alpha\n      }\n    }\n\n    val test_PercentileRanking = rank_in\n\n    println(\"The best model was trained with ranking = \" + bestRanking + \" and rank = \" + bestRank + \" and lambda = \" + bestLambda\n      + \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is \" + test_PercentileRanking + \".\")","user":"anonymous","dateUpdated":"2018-12-14T09:44:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"ranks: List[Int] = List(20)\nlambdas: List[Double] = List(0.01)\nalphas: List[Int] = List(40)\nnumIters: List[Int] = List(10, 15)\nbestModel: Option[org.apache.spark.mllib.recommendation.MatrixFactorizationModel] = None\nbestRanking: Double = 99.0\nbestRank: Int = 0\nbestLambda: Double = -1.0\nbestNumIter: Int = -1\nbestAlpha: Int = 0\nPercentil Ranking = 0.5085577036749159 for the model trained with rank = 20, lambda = 0.01, alpha = 40, and numIter = 10.\norg.apache.spark.SparkException: Job 188 cancelled part of cancelled job group zeppelin-2DXJHN6PK-20181213-032739_1279375246\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1738)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1993)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n  at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n  at org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1711)\n  at org.apache.spark.ml.recommendation.ALS$.org$apache$spark$ml$recommendation$ALS$$computeFactors(ALS.scala:1652)\n  at org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:982)\n  at org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:969)\n  at scala.collection.immutable.Range.foreach(Range.scala:160)\n  at org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:969)\n  at org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:255)\n  at org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:428)\n  at org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:447)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(<console>:125)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply(<console>:124)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply(<console>:124)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply$mcVI$sp(<console>:124)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply(<console>:124)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply(<console>:124)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1.apply$mcVD$sp(<console>:124)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1.apply(<console>:124)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1.apply(<console>:124)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$1.apply$mcVI$sp(<console>:124)\n  at $anonfun$1.apply(<console>:124)\n  at $anonfun$1.apply(<console>:124)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  ... 56 elided\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=134","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=135","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=136","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=137","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=138","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=139","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=140","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=141","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=142","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=143","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=144","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=145","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=146","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=147","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=148","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=149","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=150","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=151","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=152","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=153","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=154","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=155","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=156","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=157","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=158","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=159","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=160","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=161","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=162","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=163","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=164","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=165","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=166","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=167","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=168","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=169","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=170","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=171","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=172","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=173","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=174","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=175","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=176","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=177","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=178","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=179","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=180","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=181","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=182","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=183","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=184","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=185","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=186","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=187","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=188"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544775004209_1738768044","id":"20181213-032739_1279375246","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T09:44:40+0000","dateFinished":"2018-12-14T09:50:15+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"text":"// run the better model\n\nval rank = 50\nval numIterations = 15\nval alpha = 100\nval lambda = 0.001\nval block = -1\nval seed = 1222L\nval implicitPrefs = true\nval model = new ALS().\nsetIterations(numIterations).\nsetBlocks(block).\nsetAlpha(alpha).\nsetLambda(lambda).\nsetRank(rank).\nsetSeed(seed).\nsetImplicitPrefs(implicitPrefs).\nrun(trainRDD)\n\n","user":"anonymous","dateUpdated":"2018-12-14T10:27:36+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=277","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=278","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=279","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=280","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=281","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=282","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=283","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=284","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=285","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=286","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=287","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=288","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=289","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=290","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=291","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=292","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=293","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=294","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=295","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=296","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=297","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=298","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=299","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=300","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=301","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=302","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=303","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=304","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=305","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=306","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=307","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=308","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=309","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=310","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=311","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=312","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=313","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=314"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544775004210_1862045483","id":"20181212-061309_180340924","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T10:13:51+0000","dateFinished":"2018-12-14T10:23:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:268","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rank: Int = 50\nnumIterations: Int = 15\nalpha: Int = 100\nlambda: Double = 0.01\nblock: Int = -1\nseed: Long = 1222\nimplicitPrefs: Boolean = true\nmodel: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@a9b1846\n"}]}},{"text":"// percentile ranking function\r\ndef expectedPercentileRanking(model: MatrixFactorizationModel, ratings: RDD[Rating]) = {\r\n    val itemFactors = model.productFeatures.collect()\r\n    val itemMatrix = new DoubleMatrix(itemFactors.map(_._2))\r\n    val imBroadCast = sc.broadcast(itemMatrix)\r\n    val itemListPerUser = ratings.groupBy(_.user).map {\r\n      case (user, ratingList) => (user, ratingList.map(rt => (rt.product, rt.rating)).toArray)\r\n    }\r\n    val rankRDD = model.userFeatures.join(itemListPerUser).map {\r\n      case (userId, (userFeatures, itemRatingList)) =>\r\n        val userVector = new DoubleMatrix(userFeatures)\r\n        val scores = imBroadCast.value.mmul(userVector) //用戶對各產品喜好分數乘積\r\n        \r\n        val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1) //A. 將同一個的產品偏好分數抓取加上Index，並由小排到大\r\n        val itemsOrderedByPref = sortedWithId.map(_._2).toSeq // B. 抓取產品分數並存成sequence\r\n        \r\n        val rankWeightedByRating = itemRatingList.map {\r\n          case (itemId, rating) =>\r\n            rating * itemsOrderedByPref.indexOf(itemId).toDouble / (itemsOrderedByPref.size - 1)\r\n        \r\n        }.sum\r\n        rankWeightedByRating\r\n    }\r\n    \r\n    val weightedRankOverAll = rankRDD.sum()\r\n    val sumWeight = ratings.map(_.rating).sum()\r\n    weightedRankOverAll / sumWeight\r\n  }","user":"anonymous","dateUpdated":"2018-12-14T09:26:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"expectedPercentileRanking: (model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel, ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating])Double\n"}]},"apps":[],"jobName":"paragraph_1544775004211_-1803670499","id":"20181213-060048_1845127949","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:22:38+0000","dateFinished":"2018-12-14T08:22:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:269"},{"text":"// \n// 1. top sale ranking.\n// 2. percentile ranking of testset item.\n// val top_sale =  whole.groupBy(\"productID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc).drop(\"Total_Amount\")\n// val topSale_List = top_sale.select(\"productID\").collect().map(_(0)).toList\n\ndef topSalePercentileRanking(TopSale: List[Any], ratings: RDD[Rating]) = {\n    \n    //val List =  whole.groupBy(\"productID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc).drop(\"Total_Amount\")\n    //val topSale_List = top_sale.select(\"productID\").collect().map(_(0)).toList\n    \n    val itemListPerUser = ratings.groupBy(_.user).map {\n      case (user, ratingList) => (user, ratingList.map(rt => (rt.product, rt.rating)).toArray)\n    }\n    val rankRDD = itemListPerUser.map {\n      case (userId, itemRatingList) =>\n\n        \n        val rankWeightedByRating = itemRatingList.map {\n          case (itemId, rating) =>\n            rating * TopSale.indexOf(itemId).toDouble / (TopSale.size - 1)\n        \n        }.sum\n        rankWeightedByRating\n    }\n    \n    val weightedRankOverAll = rankRDD.sum()\n    val sumWeight = ratings.map(_.rating).sum()\n    weightedRankOverAll / sumWeight\n  }\n\n\n\n","user":"anonymous","dateUpdated":"2018-12-14T09:03:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[{"type":"TEXT","data":"topSalePercentileRanking: (TopSale: List[Any], ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating])Double\n"}]},"apps":[],"jobName":"paragraph_1544775004212_1344424095","id":"20181214-073100_1799197759","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T08:59:55+0000","dateFinished":"2018-12-14T08:59:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:270"},{"text":" topSalePercentileRanking(topSale_List,testRDD)","user":"anonymous","dateUpdated":"2018-12-14T10:22:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544777909538_1265013353","id":"20181214-085829_1387069036","dateCreated":"2018-12-14T08:58:29+0000","dateStarted":"2018-12-14T10:22:12+0000","dateFinished":"2018-12-14T10:23:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:271","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=315","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=316"],"interpreterSettingId":"spark"}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res189: Double = 0.030574923758831238\n"}]}},{"text":"/*\nval top_recommendRDD = top_recommend.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\n\nval rank_out = expectedPercentileRanking(model, top_recommendRDD)\n*/\n","user":"anonymous","dateUpdated":"2018-12-14T08:11:07+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"top_recommendRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[519] at map at <console>:77\nrank_out: org.apache.spark.rdd.RDD[Unit] = MapPartitionsRDD[526] at map at <console>:110\n"}]},"apps":[],"jobName":"paragraph_1544775004212_77764781","id":"20181213-092103_1791341371","dateCreated":"2018-12-14T08:10:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:272"},{"text":"\r\nval rank_in = expectedPercentileRanking(model, trainRDD)\r\nval rank_out = expectedPercentileRanking(model, testRDD)\r\n","user":"anonymous","dateUpdated":"2018-12-14T10:23:43+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544775004213_-1833228292","id":"20181212-070031_1706859906","dateCreated":"2018-12-14T08:10:04+0000","dateStarted":"2018-12-14T10:23:44+0000","dateFinished":"2018-12-14T10:26:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:273","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rank_in: Double = 0.4929051438196487\nrank_out: Double = 0.4953121628063027\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=317","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=318","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=319","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=320","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=321","http://ip-172-31-45-73.ec2.internal:4040/jobs/job?id=322"],"interpreterSettingId":"spark"}}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544778917035_-1638503254","id":"20181214-091517_840293925","dateCreated":"2018-12-14T09:15:17+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:274"}],"name":"MAP","id":"2DXJHN6PK","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}