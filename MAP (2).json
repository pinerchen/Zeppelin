{"paragraphs":[{"text":"%dep\r\nz.load(\"org.scalanlp:jblas:1.2.1\")\r\n","user":"anonymous","dateUpdated":"2018-12-14T05:24:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@3fc1926d\n"}]},"apps":[],"jobName":"paragraph_1544765040394_1190640346","id":"20181212-015843_1688763497","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:24:34+0000","dateFinished":"2018-12-14T05:24:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:24040"},{"text":"// 导入jblas库中的矩阵类\nimport org.jblas.DoubleMatrix\n// 定义相似度函数\ndef cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {\n    vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())\n}\n","user":"anonymous","dateUpdated":"2018-12-14T05:24:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.jblas.DoubleMatrix\ncosineSimilarity: (vec1: org.jblas.DoubleMatrix, vec2: org.jblas.DoubleMatrix)Double\n"}]},"apps":[],"jobName":"paragraph_1544765040401_550984511","id":"20181212-015920_1065716449","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:24:39+0000","dateFinished":"2018-12-14T05:25:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24041"},{"text":"import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n\nimport org.apache.spark.ml.fpm.FPGrowth\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.mllib.evaluation.RankingMetrics\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.explode\nimport org.apache.spark.sql.{DataFrame, Dataset,SparkSession}\n\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.sql.Row\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.rdd.RDD","user":"anonymous","dateUpdated":"2018-12-14T05:24:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport org.apache.spark.ml.fpm.FPGrowth\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.mllib.evaluation.RankingMetrics\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.explode\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.sql.Row\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.rdd.RDD\n"}]},"apps":[],"jobName":"paragraph_1544765040402_-1855848906","id":"20181212-015934_389368932","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:24:46+0000","dateFinished":"2018-12-14T05:25:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24042"},{"text":"//raw dataset\nval transaction = spark.read.option(\"header\",\"true\").csv(\"s3://input-smart-find/Quotations-110818-S.csv\").select($\"Quotation: Quotation ID\".alias(\"user\"), $\"Product: Product Family\".alias(\"product\"),$\"Display Quantity\".alias(\"number\")).groupBy(\"user\",\"product\").agg(sum(\"number\").alias(\"rating\")).toDF()\n val product_description = spark.read.option(\"header\",\"true\").csv(\"s3://input-smart-find/products - 110818-s.csv\").select($\"Product Family\".alias(\"product\"), $\"Product Description\".alias(\"product_description\"))\n\nval userIndexer = new StringIndexer().setInputCol(\"user\").setOutputCol(\"userID\")\nval indexed = userIndexer.fit(transaction).transform(transaction)\nval productIndexer = new StringIndexer().setInputCol(\"product\").setOutputCol(\"productID\")\nval whole = productIndexer.fit(indexed).transform(indexed)\n//whole.printSchema()\n//whole.show(10)\nval transform = whole.select($\"userID\".cast(\"int\"),$\"productID\".cast(\"int\"),$\"rating\")\nval table = transform.groupBy(\"userID\",\"productID\").agg(sum(\"rating\").alias(\"rating\")).toDF()","user":"anonymous","dateUpdated":"2018-12-14T05:24:46+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"transaction: org.apache.spark.sql.DataFrame = [user: string, product: string ... 1 more field]\nproduct_description: org.apache.spark.sql.DataFrame = [product: string, product_description: string]\nuserIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_c3e956782e34\nindexed: org.apache.spark.sql.DataFrame = [user: string, product: string ... 2 more fields]\nproductIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_df402618725b\nwhole: org.apache.spark.sql.DataFrame = [user: string, product: string ... 3 more fields]\ntransform: org.apache.spark.sql.DataFrame = [userID: int, productID: int ... 1 more field]\ntable: org.apache.spark.sql.DataFrame = [userID: int, productID: int ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=0","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=1","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=2","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=3"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544765040403_-132785592","id":"20181212-020008_856232412","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:25:12+0000","dateFinished":"2018-12-14T05:26:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24043"},{"text":"// table.count() //140062\nval numUsers = table.select(table.col(\"userID\")).distinct().count() \nval numProducts = table.select(table.col(\"productID\")).distinct().count()\n//numUsers //33984\n// numProducts //24209\n\n/*\n+------+---------+\n|unique|unique   |\n|user  |product  |\n+------+---------+\n| 33984|    24209|\n+------+---------+\n\n\n\n*/\n","user":"anonymous","dateUpdated":"2018-12-14T05:25:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"numUsers: Long = 33984\nnumProducts: Long = 24209\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=4","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=5"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544765040403_1449750929","id":"20181212-023543_1975592944","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:25:18+0000","dateFinished":"2018-12-14T05:28:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24044"},{"text":"// filter某用戶在某產品購買數量最多的紀錄\n//whole.sort($\"rating\".desc).show \n/*\n+----------+----------+-------+-------+---------+\n|      user|   product| rating| userID|productID|\n+----------+----------+-------+-------+---------+\n|Q-00030883|10QYPAT1EU|30000.0|14060.0|     26.0|\n|Q-00063632|61B7JAT6EU|30000.0|17747.0|     60.0|\n|Q-00063632|61CAKAT1EU|30000.0|17747.0|    207.0|\n|Q-00030883|10M4S00R00|30000.0|14060.0|  10806.0|\n|Q-00068476|20LTS6P200|29200.0|10726.0|  23877.0|\n+----------+----------+-------+-------+---------+\n*/\n\n\n// filter 消費數量最高的前10大用戶\n//whole.groupBy(\"userID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc).show\n/*\n+------+------------+\n|userID|Total_Amount|\n+------+------------+\n|   3.0|    537000.0|\n|  54.0|    344450.0|\n| 282.0|    250800.0|\n| 334.0|    230800.0|\n|  71.0|    173000.0|\n| 719.0|    170800.0|\n|1054.0|    160000.0|\n| 685.0|    158000.0|\n| 707.0|    154000.0|\n| 760.0|    148000.0|\n+------+------------+\n\n*/\n\n// top 10 sale\nval top_sale =  whole.groupBy(\"productID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc).show\n/*\n+---------+------------+\n|productID|Total_Amount|\n+---------+------------+\n|      0.0|    577598.0|\n|      1.0|    492920.0|\n|      5.0|    288709.0|\n|      4.0|    272528.0|\n|      3.0|    262920.0|\n|      8.0|    236287.0|\n|     18.0|    228088.0|\n|     11.0|    221678.0|\n|      2.0|    213747.0|\n|    153.0|    201092.0|\n+---------+------------+\n\n*/","user":"anonymous","dateUpdated":"2018-12-14T07:25:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544765040404_1352539787","id":"20181212-020233_1752433362","dateCreated":"2018-12-14T05:24:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:24045"},{"text":"val unique_product = table.dropDuplicates(\"productID\") //過濾單一產品\nval unique_user = table.dropDuplicates(\"userID\") //過濾單一用戶\nval join = unique_user.union(unique_product) //兩個資料表append\nval unique_product_user = join.dropDuplicates() //重複record drop掉 -> 作為train set\nval numUsers = unique_product_user.select(unique_product_user.col(\"userID\")).distinct().count() //確認數據有相同數量unique user\nval numProducts = unique_product_user.select(unique_product_user.col(\"productID\")).distinct().count() //確認數據有相同數量unique product\n//numUsers\n//numProducts","user":"anonymous","dateUpdated":"2018-12-14T05:26:06+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"unique_product: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\nunique_user: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\njoin: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\nunique_product_user: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\nnumUsers: Long = 33984\nnumProducts: Long = 24209\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=6","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=7"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544765040405_-1878828905","id":"20181213-021529_782513074","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:26:42+0000","dateFinished":"2018-12-14T05:30:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24046"},{"text":"val test = table.except(unique_product_user)\nval train = unique_product_user\n//val numTrain = train.count()\n// val numTest = test.count()\n//test.select(unique_product_user.col(\"userID\")).distinct().count()\n//test.select(unique_product_user.col(\"productID\")).distinct().count()\n// train: 88943\n// test: 51057","user":"anonymous","dateUpdated":"2018-12-14T07:26:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1544765040405_-516664298","id":"20181213-030530_760267561","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T07:26:44+0000","dateFinished":"2018-12-14T07:26:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24047"},{"text":"// val top_recommend = test.drop(\"productID\").withColumn(\"productID\", lit(\"0.0\").cast(\"Int\")).drop(\"rating\").withColumn(\"rating\", lit(1.0).cast(\"Double\"))\n//top_recommend.show(1)","user":"anonymous","dateUpdated":"2018-12-14T07:26:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"top_recommend: org.apache.spark.sql.DataFrame = [userID: int, productID: int ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1544765040407_-325340412","id":"20181213-091003_417153930","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:30:14+0000","dateFinished":"2018-12-14T05:30:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24048"},{"text":"// val training = train.union(unique_product_tr.toDF())\n\nval trainRDD = train.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\n\nval testRDD = test.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\n","user":"anonymous","dateUpdated":"2018-12-14T07:26:52+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[579] at map at <console>:76\ntestRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[604] at map at <console>:75\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=69"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544765040407_785211727","id":"20181212-043339_2062055690","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T07:26:52+0000","dateFinished":"2018-12-14T07:28:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24049"},{"text":"def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], n: Long): Double = {\r\n    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))\r\n    val predictionsAndRatings = predictions.map(x => ((x.user, x.product), x.rating))\r\n      .join(data.map(x => ((x.user, x.product), x.rating)))\r\n      .values\r\n    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).reduce(_ + _) / n)\r\n  }","user":"anonymous","dateUpdated":"2018-12-14T05:26:46+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"computeRmse: (model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], n: Long)Double\n"}]},"apps":[],"jobName":"paragraph_1544765040422_-1961613060","id":"20181212-052939_2138673281","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:30:15+0000","dateFinished":"2018-12-14T05:30:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24050"},{"text":"val ranks = List(20,50)\n    val lambdas = List(0.01,0.001)\n    val alpha = List(1,40,100)\n    val numIters = List(10, 15)\n    var bestModel: Option[MatrixFactorizationModel] = None\n    var bestRanking = Double.MaxValue\n    var bestRank = 0\n    var bestLambda = -1.0\n    var bestNumIter = -1\n    var bestAlpha = 0\n\n    for (rank <- ranks; lambda <- lambdas; numIter <- numIters; alpha <- alpha ) {\n      val model = ALS.trainImplicit(trainRDD, rank, numIter, lambda, alpha)\n      val rank_in = expectedPercentileRanking(model, trainRDD)\n      println(\"Percentil Ranking = \" + rank_in + \" for the model trained with rank = \"\n        + rank + \n        \", lambda = \" + lambda + \n        \", alpha = \" + alpha + \n        \", and numIter = \" + numIter + \".\")\n      if (rank_in < bestRanking) {\n        bestModel = Some(model)\n        bestRanking = rank_in\n        bestRank = rank\n        bestLambda = lambda\n        bestNumIter = numIter\n        bestAlpha = alpha\n      }\n    }\n\n    val test_PercentileRanking = rank_in\n\n    println(\"The best model was trained with ranking = \" + bestRanking + \" and rank = \" + bestRank + \" and lambda = \" + bestLambda\n      + \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is \" + test_PercentileRanking + \".\")","user":"anonymous","dateUpdated":"2018-12-14T05:29:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"ranks: List[Int] = List(20, 50)\nlambdas: List[Double] = List(0.01, 0.001)\nalpha: List[Int] = List(1, 40, 100)\nnumIters: List[Int] = List(10, 15)\nbestModel: Option[org.apache.spark.mllib.recommendation.MatrixFactorizationModel] = None\nbestRanking: Double = 1.7976931348623157E308\nbestRank: Int = 0\nbestLambda: Double = -1.0\nbestNumIter: Int = -1\nbestAlpha: Int = 0\n<console>:73: error: not found: value trainRDD\n             val model = ALS.trainImplicit(trainRDD, rank, numIter, lambda, alpha)\n                                           ^\n<console>:74: error: not found: value expectedPercentileRanking\n             val rank_in = expectedPercentileRanking(model, trainRDD)\n                           ^\n<console>:74: error: not found: value trainRDD\n             val rank_in = expectedPercentileRanking(model, trainRDD)\n                                                            ^\n"}]},"apps":[],"jobName":"paragraph_1544765040424_539189437","id":"20181213-032739_1279375246","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:30:16+0000","dateFinished":"2018-12-14T05:30:20+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:24051"},{"text":"\nval rank = 50\nval numIterations = 10\nval alpha = 100\nval lambda = 0.01\nval block = -1\nval seed = 1222L\nval implicitPrefs = true\nval model = new ALS().\nsetIterations(numIterations).\nsetBlocks(block).\nsetAlpha(alpha).\nsetLambda(lambda).\nsetRank(rank).\nsetSeed(seed).\nsetImplicitPrefs(implicitPrefs).\nrun(trainRDD)\n\n","user":"anonymous","dateUpdated":"2018-12-14T05:34:09+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rank: Int = 50\nnumIterations: Int = 10\nalpha: Int = 100\nlambda: Double = 0.01\nblock: Int = -1\nseed: Long = 1222\nimplicitPrefs: Boolean = true\nmodel: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@1dc4a416\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=13","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=14","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=15","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=16","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=17","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=18","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=19","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=20","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=21","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=22","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=23","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=24","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=25","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=26","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=27","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=28","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=29","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=30","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=31","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=32","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=33","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=34","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=35","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=36","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=37","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=38","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=39","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=40"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544765040425_1082841428","id":"20181212-061309_180340924","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T05:36:31+0000","dateFinished":"2018-12-14T05:40:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24052"},{"text":"/* RMSE validation\nval ranks = List(12,20)\n    val lambdas = List(0.1,0.01)\n    val alpha = List(1,40)\n    val numIters = List(10, 15)\n    var bestModel: Option[MatrixFactorizationModel] = None\n    var bestValidationRmse = Double.MaxValue\n    var bestRank = 0\n    var bestLambda = -1.0\n    var bestNumIter = -1\n    var bestAlpha = 0\n    for (rank <- ranks; lambda <- lambdas; numIter <- numIters; alpha <- alpha) {\n      val model = ALS.trainImplicit(trainRDD, rank, numIter, lambda, alpha)\n      val validationRmse = computeRmse(model, testRDD, numTest)\n      println(\"RMSE (validation) = \" + validationRmse + \" for the model trained with rank = \"\n        + rank + \n        \", lambda = \" + lambda + \n        \", alpha = \" + alpha + \n        \", and numIter = \" + numIter + \".\")\n      if (validationRmse < bestValidationRmse) {\n        bestModel = Some(model)\n        bestValidationRmse = validationRmse\n        bestRank = rank\n        bestLambda = lambda\n        bestNumIter = numIter\n        bestAlpha = alpha\n      }\n    }\n\n    val testRmse = computeRmse(bestModel.get, testRDD, numTest)\n\n    println(\"The best model was trained with rank = \" + bestRank + \" and lambda = \" + bestLambda\n      + \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is \" + testRmse + \".\")\n      \n    */","user":"anonymous","dateUpdated":"2018-12-14T05:24:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"ranks: List[Int] = List(12, 20)\nlambdas: List[Double] = List(0.1, 0.01)\nalpha: List[Int] = List(1, 40)\nnumIters: List[Int] = List(10, 15)\nbestModel: Option[org.apache.spark.mllib.recommendation.MatrixFactorizationModel] = None\nbestValidationRmse: Double = 1.7976931348623157E308\nbestRank: Int = 0\nbestLambda: Double = -1.0\nbestNumIter: Int = -1\nbestAlpha: Int = 0\nRMSE (validation) = 751.1288744158197 for the model trained with rank = 12, lambda = 0.1, alpha = 1, and numIter = 10.\nRMSE (validation) = 747.493165660131 for the model trained with rank = 12, lambda = 0.1, alpha = 40, and numIter = 10.\nRMSE (validation) = 746.9286561845882 for the model trained with rank = 12, lambda = 0.1, alpha = 1, and numIter = 15.\nRMSE (validation) = 747.5758069451114 for the model trained with rank = 12, lambda = 0.1, alpha = 40, and numIter = 15.\nRMSE (validation) = 746.619456766447 for the model trained with rank = 12, lambda = 0.01, alpha = 1, and numIter = 10.\norg.apache.spark.SparkException: Job 213 cancelled part of cancelled job group zeppelin-2DX12C71Q-20181212-051721_1400508414\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1738)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1993)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n  at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n  at org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1711)\n  at org.apache.spark.ml.recommendation.ALS$.org$apache$spark$ml$recommendation$ALS$$computeFactors(ALS.scala:1652)\n  at org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:982)\n  at org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:969)\n  at scala.collection.immutable.Range.foreach(Range.scala:160)\n  at org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:969)\n  at org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:255)\n  at org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:428)\n  at org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:447)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(<console>:103)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply(<console>:102)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply(<console>:102)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply$mcVI$sp(<console>:102)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply(<console>:102)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply(<console>:102)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1.apply$mcVD$sp(<console>:102)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1.apply(<console>:102)\n  at $anonfun$1$$anonfun$apply$mcVI$sp$1.apply(<console>:102)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$1.apply$mcVI$sp(<console>:102)\n  at $anonfun$1.apply(<console>:102)\n  at $anonfun$1.apply(<console>:102)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  ... 56 elided\n"}]},"apps":[],"jobName":"paragraph_1544765040425_1472128490","id":"20181212-051721_1400508414","dateCreated":"2018-12-14T05:24:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:24053"},{"text":"// popular item rating\r\n// val whole.groupBy(\"productID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc)\r\ndef expectedPercentileRanking(model: MatrixFactorizationModel, ratings: RDD[Rating]) = {\r\n    val itemFactors = model.productFeatures.collect()\r\n    val itemMatrix = new DoubleMatrix(itemFactors.map(_._2))\r\n    val imBroadCast = sc.broadcast(itemMatrix)\r\n    val itemListPerUser = ratings.groupBy(_.user).map {\r\n      case (user, ratingList) => (user, ratingList.map(rt => (rt.product, rt.rating)).toArray)\r\n    }\r\n    val rankRDD = model.userFeatures.join(itemListPerUser).map {\r\n      case (userId, (userFeatures, itemRatingList)) =>\r\n        val userVector = new DoubleMatrix(userFeatures)\r\n        val scores = imBroadCast.value.mmul(userVector) //用戶對各產品喜好分數乘積\r\n        \r\n        val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1) //A. 將同一個的產品偏好分數抓取加上Index，並由小排到大\r\n        val itemsOrderedByPref = sortedWithId.map(_._2).toSeq // B. 抓取產品分數並存成sequence\r\n        \r\n        val rankWeightedByRating = itemRatingList.map {\r\n          case (itemId, rating) =>\r\n            rating * itemsOrderedByPref.indexOf(itemId).toDouble / (itemsOrderedByPref.size - 1)\r\n        \r\n        }.sum\r\n        rankWeightedByRating\r\n    }\r\n    \r\n    val weightedRankOverAll = rankRDD.sum()\r\n    val sumWeight = ratings.map(_.rating).sum()\r\n    weightedRankOverAll / sumWeight\r\n  }","user":"anonymous","dateUpdated":"2018-12-14T06:40:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"expectedPercentileRanking: (model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel, ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating])Double\n"}]},"apps":[],"jobName":"paragraph_1544765040427_-721891912","id":"20181213-060048_1845127949","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T06:40:22+0000","dateFinished":"2018-12-14T06:40:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24054"},{"text":"// \n// 1. top sale ranking.\n// 2. percentile ranking of testset item.\nval top_sale =  whole.groupBy(\"productID\").agg(sum(\"rating\").alias(\"Total_Amount\")).sort($\"Total_Amount\".desc).drop(\"Total_Amount\")\nval topSaleRDD = top_sale.rdd\n  \nval topSale_sortedWithId = topSaleRDD.data.zipWithIndex() \n\n","user":"anonymous","dateUpdated":"2018-12-14T07:45:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544772660211_-886828830","id":"20181214-073100_1799197759","dateCreated":"2018-12-14T07:31:00+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:24055","dateFinished":"2018-12-14T07:45:12+0000","dateStarted":"2018-12-14T07:45:02+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"top_sale: org.apache.spark.sql.DataFrame = [productID: double]\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n"}]}},{"text":"\nval top_recommendRDD = top_recommend.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\n\nval rank_out = expectedPercentileRanking(model, top_recommendRDD)\n","user":"anonymous","dateUpdated":"2018-12-14T06:33:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"top_recommendRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[519] at map at <console>:77\nrank_out: org.apache.spark.rdd.RDD[Unit] = MapPartitionsRDD[526] at map at <console>:110\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=54","http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=55"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544765040427_-509907393","id":"20181213-092103_1791341371","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T06:33:54+0000","dateFinished":"2018-12-14T06:34:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:24056"},{"text":"val rank_in = expectedPercentileRanking(model, trainRDD)\r\nval rank_out = expectedPercentileRanking(model, testRDD)\r\nval rank_out = expectedPercentileRanking(model, top_recommendRDD)","user":"anonymous","dateUpdated":"2018-12-14T07:30:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.OutOfMemoryError: GC overhead limit exceeded\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-45-171.ec2.internal:4040/jobs/job?id=70"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544765040428_816573375","id":"20181212-070031_1706859906","dateCreated":"2018-12-14T05:24:00+0000","dateStarted":"2018-12-14T07:30:53+0000","dateFinished":"2018-12-14T07:32:27+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:24057"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544772653331_1252140599","id":"20181214-073053_576082838","dateCreated":"2018-12-14T07:30:53+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:24058"}],"name":"MAP","id":"2E18K3XM1","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}