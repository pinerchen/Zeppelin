{"paragraphs":[{"text":"*%dep\r\nz.load(\"org.scalanlp:jblas:1.2.1\")","user":"anonymous","dateUpdated":"2018-12-07T03:32:03+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@54ad52b5\n"}]},"apps":[],"jobName":"paragraph_1544146833679_-2000253602","id":"20181207-014033_1320992749","dateCreated":"2018-12-07T01:40:33+0000","dateStarted":"2018-12-07T01:40:44+0000","dateFinished":"2018-12-07T01:40:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:29989"},{"text":"// 导入jblas库中的矩阵类\nimport org.jblas.DoubleMatrix\n// 定义相似度函数\ndef cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {\n    vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())\n}\n","user":"anonymous","dateUpdated":"2018-12-07T01:41:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.jblas.DoubleMatrix\ncosineSimilarity: (vec1: org.jblas.DoubleMatrix, vec2: org.jblas.DoubleMatrix)Double\n"}]},"apps":[],"jobName":"paragraph_1544146874569_914034379","id":"20181207-014114_1651130668","dateCreated":"2018-12-07T01:41:14+0000","dateStarted":"2018-12-07T01:41:21+0000","dateFinished":"2018-12-07T01:41:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29990"},{"text":"import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n\nimport org.apache.spark.ml.fpm.FPGrowth\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.explode\nimport org.apache.spark.sql.{DataFrame, Dataset,SparkSession}","user":"anonymous","dateUpdated":"2018-12-07T01:52:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport org.apache.spark.ml.fpm.FPGrowth\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.explode\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n"}]},"apps":[],"jobName":"paragraph_1544147522629_1801181089","id":"20181207-015202_1846790921","dateCreated":"2018-12-07T01:52:02+0000","dateStarted":"2018-12-07T01:52:55+0000","dateFinished":"2018-12-07T01:52:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29991"},{"text":"import org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.sql.Row\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.rdd.RDD","user":"anonymous","dateUpdated":"2018-12-07T01:45:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.sql.Row\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.rdd.RDD\n"}]},"apps":[],"jobName":"paragraph_1544146826546_408459660","id":"20181206-015555_1654254797","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T01:45:42+0000","dateFinished":"2018-12-07T01:45:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29992"},{"text":"import org.apache.spark.sql.functions._\nval transaction = spark.read.option(\"header\",\"true\").csv(\"s3://input-smart-find/Quotations-110818-S.csv\").select($\"Quotation: Quotation ID\".alias(\"user\"), $\"Product: Product Family\".alias(\"product\"),$\"Display Quantity\".alias(\"number\")).groupBy(\"user\",\"product\").agg(sum(\"number\").alias(\"rating\")).toDF()\n\n","user":"anonymous","dateUpdated":"2018-12-07T01:48:06+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\ntransaction: org.apache.spark.sql.DataFrame = [user: string, product: string ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=0"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826550_-536227741","id":"20181206-031602_2120750532","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T01:48:06+0000","dateFinished":"2018-12-07T01:48:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29993"},{"text":"// load product description dataset\n// val product_description = spark.read.option(\"header\",\"true\").csv(\"s3://input-smart-find/products - 110818-s.csv\").select($\"Product Family\".alias(\"product\"), $\"Product Description\".alias(\"product_description\"))","user":"anonymous","dateUpdated":"2018-12-07T01:48:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"product_description: org.apache.spark.sql.DataFrame = [product: string, product_description: string]\n"}]},"apps":[],"jobName":"paragraph_1544146826551_-1295155742","id":"20181206-050321_333714499","dateCreated":"2018-12-07T01:40:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:29994"},{"text":"// create unique userID and productID\nval userIndexer = new StringIndexer().setInputCol(\"user\").setOutputCol(\"userID\")\nval indexed = userIndexer.fit(transaction).transform(transaction)\nval productIndexer = new StringIndexer().setInputCol(\"product\").setOutputCol(\"productID\")\nval whole = productIndexer.fit(indexed).transform(indexed)\nwhole.printSchema()\nwhole.show(10)\nval transform = whole.select($\"userID\".cast(\"int\"),$\"productID\".cast(\"int\"),$\"rating\")\nval table = transform.groupBy(\"userID\",\"productID\").agg(sum(\"rating\").alias(\"rating\")).toDF()","user":"anonymous","dateUpdated":"2018-12-07T02:09:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"userIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_623edad85243\nindexed: org.apache.spark.sql.DataFrame = [user: string, product: string ... 2 more fields]\nproductIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_edee5900ea21\nwhole: org.apache.spark.sql.DataFrame = [user: string, product: string ... 3 more fields]\nroot\n |-- user: string (nullable = true)\n |-- product: string (nullable = true)\n |-- rating: double (nullable = true)\n |-- userID: double (nullable = false)\n |-- productID: double (nullable = false)\n\n+----------+----------+------+-------+---------+\n|      user|   product|rating| userID|productID|\n+----------+----------+------+-------+---------+\n|Q-00006876|10MQS79500| 150.0| 3545.0|  12144.0|\n|Q-00004324|4Z90Q25510| 500.0| 7696.0|  21270.0|\n|Q-00018356|10LKPAT6EU|  16.0| 5969.0|   1140.0|\n|Q-00028520|4X30H56828|   5.0|14868.0|    231.0|\n|Q-00009760|4XH0N04885| 100.0|13316.0|    746.0|\n|Q-00022082|   0A36536|  50.0| 2245.0|     61.0|\n|Q-00063689|   39Y7937|   4.0| 4971.0|    123.0|\n|Q-00017806|10QYPAT1UK| 500.0|12599.0|     29.0|\n|Q-00074595|7Y37A01086|   1.0|  591.0|    452.0|\n|Q-00054445|61A6MAT3EU|  15.0|17770.0|      7.0|\n+----------+----------+------+-------+---------+\nonly showing top 10 rows\n\ntransform: org.apache.spark.sql.DataFrame = [userID: int, productID: int ... 1 more field]\ntable: org.apache.spark.sql.DataFrame = [userID: int, productID: int ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=8","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=9","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=10"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826551_-556908582","id":"20181206-032238_531768591","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T02:09:20+0000","dateFinished":"2018-12-07T02:10:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29995"},{"text":"table.describe().show()","user":"anonymous","dateUpdated":"2018-12-07T02:11:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-----------------+-----------------+------------------+\n|summary|           userID|        productID|            rating|\n+-------+-----------------+-----------------+------------------+\n|  count|           140062|           140062|            140062|\n|   mean|6952.679763247705| 3182.26753866145|211.59446530822063|\n| stddev|8414.775901249903|5448.523067208439| 704.5583437444309|\n|    min|                0|                0|               0.0|\n|    max|            33983|            24208|           30000.0|\n+-------+-----------------+-----------------+------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=11"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544148454029_-124136739","id":"20181207-020734_1436817550","dateCreated":"2018-12-07T02:07:34+0000","dateStarted":"2018-12-07T02:11:28+0000","dateFinished":"2018-12-07T02:12:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29996"},{"text":"val productMapping = whole.select($\"product\",$\"productID\")\nval userMapping = whole.select($\"user\",$\"userID\")\nval product_name = productMapping.rdd.map(r => (r(1),r(0))).collectAsMap()\nval user_name = userMapping.rdd.map(r => (r(1),r(0))).collectAsMap()\nproduct_name(12572)\nuser_name(1)","user":"anonymous","dateUpdated":"2018-12-07T02:14:17+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":88,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"productMapping: org.apache.spark.sql.DataFrame = [product: string, productID: double]\nuserMapping: org.apache.spark.sql.DataFrame = [user: string, userID: double]\nproduct_name: scala.collection.Map[Any,Any] = Map(12572.0 -> 20L6SC4G00, 3672.0 -> 7X15A02BEA, 6938.0 -> 20EQS6EK00, 6220.0 -> 30BGS1D300, 12886.0 -> 20LJS15N00, 3331.0 -> 0B47039, 5879.0 -> 20HMS0T800, 2990.0 -> 30BH0002GE, 5538.0 -> 20JH002RIX, 5197.0 -> 20JRS0LY00, 18090.0 -> 10R50000UK, 24038.0 -> 20LA001SMX, 18404.0 -> 10MAS4KW00, 20952.0 -> 20KH006JSC, 17749.0 -> 00MJ101, 23697.0 -> ZA2K0132RU, 18063.0 -> 7SD7A05750, 14797.0 -> 953292G, 17408.0 -> 20MAS0FR00, 20611.0 -> 10NNS1DJ00, 24011.0 -> 20M7S01L00, 23356.0 -> 20HES6W600, 17722.0 -> 20LD002HMH, 17004.0 -> 30B6S22H00, 20270.0 -> 10MNS3C200, 19615.0 -> 20HLS32D00, 23670.0 -> 20HES2Y200, 10715.0 -> 20J5S2BF00, 16663.0 -> 20JB0017PG, 19929.0 -> 20K5S37309, 19274.0 -> 20HH0016MB, 19211.0 -> 20LXS2D600, 10374.0 -> 20J5S1L200, 12922...user_name: scala.collection.Map[Any,Any] = Map(12572.0 -> Q-00061484, 29520.0 -> Q-00066752, 3672.0 -> Q-00062586, 6938.0 -> Q-00017424, 32068.0 -> Q-00041051, 6220.0 -> Q-00055408, 12886.0 -> Q-00005916, 32382.0 -> Q-00074731, 3331.0 -> Q-00071843, 5879.0 -> Q-00061896, 31727.0 -> Q-00041897, 2990.0 -> Q-00037053, 32041.0 -> Q-00006028, 31386.0 -> Q-00060865, 5538.0 -> Q-00068442, 33934.0 -> Q-00073795, 25097.0 -> Q-00031200, 27645.0 -> Q-00031969, 24379.0 -> Q-00070108, 31045.0 -> Q-00047439, 5197.0 -> Q-00040088, 33593.0 -> Q-00062548, 18090.0 -> Q-00042313, 24693.0 -> Q-00068425, 27304.0 -> Q-00035694, 24038.0 -> Q-00045374, 18404.0 -> Q-00063263, 20952.0 -> Q-00014478, 33252.0 -> Q-00025044, 17749.0 -> Q-00003973, 24352.0 -> Q-00062613, 27618.0 -> Q-00054686, 23697.0 -> Q-00013539,...res29: Any = 20L6SC4G00\nres30: Any = Q-00024173\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=12","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=13"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826552_-976534572","id":"20181206-034434_98546566","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T02:14:17+0000","dateFinished":"2018-12-07T02:15:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29997"},{"text":"//join product id with description\n// val product_join = product.join(product_description,\"product\")    //有null description's product.\n// product_join.printSchema()\n// product_join.filter(\"product_description is null\").show()\n","user":"anonymous","dateUpdated":"2018-12-07T02:12:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res147: Any = NoteBook TP T480 8G 1TB W10P\n"}]},"apps":[],"jobName":"paragraph_1544146826552_-491755969","id":"20181206-035821_1799462590","dateCreated":"2018-12-07T01:40:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:29998"},{"text":"def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], n: Long): Double = {\r\n    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))\r\n    val predictionsAndRatings = predictions.map(x => ((x.user, x.product), x.rating))\r\n      .join(data.map(x => ((x.user, x.product), x.rating)))\r\n      .values\r\n    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).reduce(_ + _) / n)\r\n  }","user":"anonymous","dateUpdated":"2018-12-07T02:16:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"computeRmse: (model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], n: Long)Double\n"}]},"apps":[],"jobName":"paragraph_1544146826553_978701059","id":"20181206-071238_122271083","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T02:16:12+0000","dateFinished":"2018-12-07T02:16:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29999"},{"text":"// split table to test and train set\nval rows: RDD[Row] = table.rdd\nval splits = table.randomSplit(Array(0.8,0.2),seed = 12345L)\nval (train,test) = (splits(0),splits(1))\nval numTrain = train.count()\nval numTest = test.count()\nprintln(\"Train Data:\" + numTrain + \" Test Data:\" + numTest)\n","user":"anonymous","dateUpdated":"2018-12-07T02:18:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[147] at rdd at <console>:68\nsplits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([userID: int, productID: int ... 1 more field], [userID: int, productID: int ... 1 more field])\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, productID: int ... 1 more field]\nnumTrain: Long = 111862\nnumTest: Long = 28200\nTrain Data:111862 Test Data:28200\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=14","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=15"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826553_-633442223","id":"20181206-023558_1686975681","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T02:18:59+0000","dateFinished":"2018-12-07T02:21:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30000"},{"text":"val tableRDD = rows.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\nval trainRDD = train.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))\n\nval testRDD = test.rdd.map(r => Rating(\n  r.getAs[Int](\"userID\"), r.getAs[Int](\"productID\"), r.getAs[Double](\"rating\")\n))","user":"anonymous","dateUpdated":"2018-12-07T02:29:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"tableRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[195] at map at <console>:69\ntrainRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[196] at map at <console>:55\ntestRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[197] at map at <console>:55\n"}]},"apps":[],"jobName":"paragraph_1544146826553_335537258","id":"20181206-024943_267374160","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T02:29:34+0000","dateFinished":"2018-12-07T02:29:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30001"},{"text":"//////// test\nimport org.apache.spark.rdd.RDD\nval ranks = List(8, 12,20)\n    val lambdas = List(0.1, 1.0, 10.0)\n    val alpha = List(1,40)\n    val numIters = List(10, 20)\n    var bestModel: Option[MatrixFactorizationModel] = None\n    var bestValidationRmse = Double.MaxValue\n    var bestRank = 0\n    var bestLambda = -1.0\n    var bestNumIter = -1\n    var bestAlpha = 0\n    for (rank <- ranks; lambda <- lambdas; numIter <- numIters; alpha <- alpha) {\n      val model = ALS.trainImplicit(trainRDD, rank, numIter, lambda, alpha)\n      val validationRmse = computeRmse(model, testRDD, numTest)\n      println(\"RMSE (validation) = \" + validationRmse + \" for the model trained with rank = \"\n        + rank + \n        \", lambda = \" + lambda + \n        \", alpha = \" + alpha + \n        \", and numIter = \" + numIter + \".\")\n      if (validationRmse < bestValidationRmse) {\n        bestModel = Some(model)\n        bestValidationRmse = validationRmse\n        bestRank = rank\n        bestLambda = lambda\n        bestNumIter = numIter\n        bestAlpha = alpha\n      }\n    }\n\n    val testRmse = computeRmse(bestModel.get, testRDD, numTest)\n\n    println(\"The best model was trained with rank = \" + bestRank + \" and lambda = \" + bestLambda\n      + \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is \" + testRmse + \".\")","user":"anonymous","dateUpdated":"2018-12-07T01:40:26+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":454,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.rdd.RDD\nranks: List[Int] = List(8, 12, 20)\nlambdas: List[Double] = List(0.1, 1.0, 10.0)\nalpha: List[Int] = List(1, 40)\nnumIters: List[Int] = List(10, 20)\nbestModel: Option[org.apache.spark.mllib.recommendation.MatrixFactorizationModel] = None\nbestValidationRmse: Double = 1.7976931348623157E308\nbestRank: Int = 0\nbestLambda: Double = -1.0\nbestNumIter: Int = -1\nbestAlpha: Int = 0\nRMSE (validation) = 2.847347274799956 for the model trained with rank = 8, lambda = 0.1, alpha = 1, and numIter = 10.\nRMSE (validation) = 2.7560624651707477 for the model trained with rank = 8, lambda = 0.1, alpha = 40, and numIter = 10.\nRMSE (validation) = 2.8466697456857677 for the model trained with rank = 8, lambda = 0.1, alpha = 1, and numIter = 20.\nRMSE (validation) = 2.7591183281554 for the model trained with rank = 8, lambda = 0.1, alpha = 40, and numIter = 20.\nRMSE (validation) = 2.8785340102361974 for the model trained with rank = 8, lambda = 1.0, alpha = 1, and numIter = 10.\nRMSE (validation) = 2.758219738464296 for the model trained with rank = 8, lambda = 1.0, alpha = 40, and numIter = 10.\nRMSE (validation) = 2.8780450803383295 for the model trained with rank = 8, lambda = 1.0, alpha = 1, and numIter = 20.\nRMSE (validation) = 2.7576616549201343 for the model trained with rank = 8, lambda = 1.0, alpha = 40, and numIter = 20.\nRMSE (validation) = 2.902682359469255 for the model trained with rank = 8, lambda = 10.0, alpha = 1, and numIter = 10.\nRMSE (validation) = 2.777477831463707 for the model trained with rank = 8, lambda = 10.0, alpha = 40, and numIter = 10.\nRMSE (validation) = 2.9026823594703015 for the model trained with rank = 8, lambda = 10.0, alpha = 1, and numIter = 20.\nRMSE (validation) = 2.7770643417427427 for the model trained with rank = 8, lambda = 10.0, alpha = 40, and numIter = 20.\nRMSE (validation) = 2.8455561110884453 for the model trained with rank = 12, lambda = 0.1, alpha = 1, and numIter = 10.\nRMSE (validation) = 2.7575965823067365 for the model trained with rank = 12, lambda = 0.1, alpha = 40, and numIter = 10.\nRMSE (validation) = 2.843354415392529 for the model trained with rank = 12, lambda = 0.1, alpha = 1, and numIter = 20.\nRMSE (validation) = 2.757702880231383 for the model trained with rank = 12, lambda = 0.1, alpha = 40, and numIter = 20.\nRMSE (validation) = 2.8769029808008164 for the model trained with rank = 12, lambda = 1.0, alpha = 1, and numIter = 10.\nRMSE (validation) = 2.7572147379353367 for the model trained with rank = 12, lambda = 1.0, alpha = 40, and numIter = 10.\nRMSE (validation) = 2.8768752097539 for the model trained with rank = 12, lambda = 1.0, alpha = 1, and numIter = 20.\nRMSE (validation) = 2.7550424895336207 for the model trained with rank = 12, lambda = 1.0, alpha = 40, and numIter = 20.\nRMSE (validation) = 2.9026823594675593 for the model trained with rank = 12, lambda = 10.0, alpha = 1, and numIter = 10.\nRMSE (validation) = 2.776787661754127 for the model trained with rank = 12, lambda = 10.0, alpha = 40, and numIter = 10.\nRMSE (validation) = 2.902682359470278 for the model trained with rank = 12, lambda = 10.0, alpha = 1, and numIter = 20.\nRMSE (validation) = 2.7757226289503696 for the model trained with rank = 12, lambda = 10.0, alpha = 40, and numIter = 20.\nRMSE (validation) = 2.842629763871692 for the model trained with rank = 20, lambda = 0.1, alpha = 1, and numIter = 10.\nRMSE (validation) = 2.760989182851731 for the model trained with rank = 20, lambda = 0.1, alpha = 40, and numIter = 10.\nRMSE (validation) = 2.841440644009583 for the model trained with rank = 20, lambda = 0.1, alpha = 1, and numIter = 20.\norg.apache.spark.SparkException: Job 1386 cancelled part of cancelled job group zeppelin-2DZ92S39F-20181206-064537_1456665003\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1738)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1993)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n  at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n  at org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1711)\n  at org.apache.spark.ml.recommendation.ALS$.org$apache$spark$ml$recommendation$ALS$$computeFactors(ALS.scala:1652)\n  at org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:972)\n  at org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:969)\n  at scala.collection.immutable.Range.foreach(Range.scala:160)\n  at org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:969)\n  at org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:255)\n  at org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:428)\n  at org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:447)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply$mcVI$sp(<console>:272)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply(<console>:271)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1$$anonfun$apply$mcVI$sp$2.apply(<console>:271)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply$mcVI$sp(<console>:271)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply(<console>:271)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$mcVD$sp$1.apply(<console>:271)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1.apply$mcVD$sp(<console>:271)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1.apply(<console>:271)\n  at $$$1741c48faa302f95dea11e2e5d7dddc$$$$w$$anonfun$1$$anonfun$apply$mcVI$sp$1.apply(<console>:271)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$1.apply$mcVI$sp(<console>:271)\n  at $anonfun$1.apply(<console>:271)\n  at $anonfun$1.apply(<console>:271)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  ... 96 elided\n"}]},"apps":[],"jobName":"paragraph_1544146826554_1250754363","id":"20181206-064537_1456665003","dateCreated":"2018-12-07T01:40:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30002"},{"text":"// feedback for RMSE\n// rank larger有幫助, 但12,20沒有表現比8,12好; rank=12 better than rank=8\n// lambda should not be 10; lambda 1.0 better than 0.1.\n// alpha 40 better than 1.\n","user":"anonymous","dateUpdated":"2018-12-07T01:40:26+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544146826555_1554890033","id":"20181206-092423_536583109","dateCreated":"2018-12-07T01:40:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30003"},{"text":"\nval rank = 12\nval numIterations = 20\nval alpha = 0.01\nval lambda = 0.01\nval block = -1\nval seed = 1222L\nval implicitPrefs = true\nval model = new ALS().\nsetIterations(numIterations).\nsetBlocks(block).\nsetAlpha(alpha).\nsetLambda(lambda).\nsetRank(rank).\nsetSeed(seed).\nsetImplicitPrefs(implicitPrefs).\nrun(tableRDD)","user":"anonymous","dateUpdated":"2018-12-07T05:42:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rank: Int = 12\nnumIterations: Int = 20\nalpha: Double = 0.01\nlambda: Double = 0.01\nblock: Int = -1\nseed: Long = 1222\nimplicitPrefs: Boolean = true\nmodel: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@1a33966b\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=112","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=113","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=114","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=115","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=116","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=117","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=118","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=119","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=120","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=121","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=122","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=123","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=124","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=125","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=126","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=127","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=128","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=129","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=130","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=131","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=132","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=133","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=134","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=135","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=136","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=137","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=138","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=139","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=140","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=141","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=142","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=143","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=144","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=145","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=146","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=147","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=148","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=149","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=150","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=151","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=152","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=153","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=154","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=155","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=156","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=157","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=158","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=159"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826555_-2049972374","id":"20181206-015929_494305696","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T05:42:11+0000","dateFinished":"2018-12-07T05:53:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30004"},{"text":"model.recommendProductsForUsers(10).take(3).foreach{\r\n      case(x,rating) =>println(rating(0))\r\n    }","user":"anonymous","dateUpdated":"2018-12-07T07:22:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Rating(10000,1,0.22199518402882795)\nRating(30000,11,0.026163587245899685)\nRating(0,24,6.046391574185831E-4)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=165"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544167349937_-1211628965","id":"20181207-072229_963563673","dateCreated":"2018-12-07T07:22:29+0000","dateStarted":"2018-12-07T07:22:50+0000","dateFinished":"2018-12-07T07:41:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30005"},{"text":"println(model.recommendProductsForUsers(1).collect())","user":"anonymous","dateUpdated":"2018-12-07T08:38:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 3879 in stage 4813.0 failed 4 times, most recent failure: Lost task 3879.3 in stage 4813.0 (TID 66859, ip-172-31-3-65.ec2.internal, executor 293): ExecutorLostFailure (executor 293 exited caused by one of the running tasks) Reason: Container marked as failed: container_1544146771303_0001_01_000554 on host: ip-172-31-3-65.ec2.internal. Exit status: 50. Diagnostics: Exception from container-launch.\nContainer id: container_1544146771303_0001_01_000554\nExit code: 50\nStack trace: ExitCodeException exitCode=50:\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:972)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:869)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:235)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:299)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:83)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nContainer exited with a non-zero exit code 50\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  ... 60 elided\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=166"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544171858654_2060044763","id":"20181207-083738_1987881250","dateCreated":"2018-12-07T08:37:38+0000","dateStarted":"2018-12-07T08:38:02+0000","dateFinished":"2018-12-07T08:58:00+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:30006"},{"text":"model.recommendProductsForUsers(5).flatMap(x => {\n      val user = x._1.toString\n      val rat = x._2\n\n      var res = List[(String,String)]()\n      for( r <- rat){\n        res=res:+(user,r.product+\",\"+r.rating)\n      }\n      res\n    }).saveAsTextFile(\"s3://input-smart-find/rec.csv\")\n","user":"anonymous","dateUpdated":"2018-12-07T09:44:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n  ... 60 elided\nCaused by: org.apache.spark.SparkException: Job 174 cancelled part of cancelled job group zeppelin-2DYVNC4HZ-20181207-091049_602704432\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1738)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1993)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n  ... 88 more\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=174"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544173849922_-170835433","id":"20181207-091049_602704432","dateCreated":"2018-12-07T09:10:49+0000","dateStarted":"2018-12-07T09:44:30+0000","dateFinished":"2018-12-07T10:11:52+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:30007"},{"text":"// test Rating RDD to dataframe\n\nval result: RDD[(Int, Array[Rating])] = model.recommendProductsForUsers(10)\nval prediction: RDD[Rating] = result.values.flatMap(ratings => ratings)\nval predictionDF = spark.createDataFrame(prediction)","user":"anonymous","dateUpdated":"2018-12-07T10:12:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544177172153_1065881171","id":"20181207-100612_871486339","dateCreated":"2018-12-07T10:06:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30008","dateFinished":"2018-12-07T10:12:19+0000","dateStarted":"2018-12-07T10:12:15+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"result: org.apache.spark.rdd.RDD[(Int, Array[org.apache.spark.mllib.recommendation.Rating])] = MapPartitionsRDD[1341] at map at MatrixFactorizationModel.scala:218\nprediction: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[1343] at flatMap at <console>:97\npredictionDF: org.apache.spark.sql.DataFrame = [user: int, product: int ... 1 more field]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544178159850_-1380798202","id":"20181207-102239_1350570454","dateCreated":"2018-12-07T10:22:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:33683","text":"predictionDF.printSchema()","dateUpdated":"2018-12-07T10:23:32+0000","dateFinished":"2018-12-07T10:22:55+0000","dateStarted":"2018-12-07T10:22:51+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- user: integer (nullable = false)\n |-- product: integer (nullable = false)\n |-- rating: double (nullable = false)\n\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544177545034_607206643","id":"20181207-101225_1357736489","dateCreated":"2018-12-07T10:12:25+0000","status":"ABORT","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:33484","text":"predictionDF.select(predictionDF.col(\"*\")).filter(\"user = 6689\").show()","dateUpdated":"2018-12-07T10:20:13+0000","dateFinished":"2018-12-07T10:22:38+0000","dateStarted":"2018-12-07T10:20:13+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 176 cancelled part of cancelled job group zeppelin-2DYVNC4HZ-20181207-101225_1357736489\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1738)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:851)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:851)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1993)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)\n  ... 60 elided\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=176"],"interpreterSettingId":"spark"}}},{"text":"\nfor (userID <- Array(1,6689,221)) {\n     val topKRecs = model.recommendProducts(userID,5)\n    println(topKRecs.mkString(\"\\n\"))\n    }","user":"anonymous","dateUpdated":"2018-12-07T08:58:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Rating(1,24,6.058286973952765E-4)\nRating(1,1814,5.21965932696925E-4)\nRating(1,185,5.194358497387072E-4)\nRating(1,771,4.982277276701149E-4)\nRating(1,23,4.980184191765063E-4)\nRating(6689,0,6.714891497169504E-5)\nRating(6689,24,4.613480661092221E-5)\nRating(6689,68,4.5938794496594903E-5)\nRating(6689,5,4.260292321602031E-5)\nRating(6689,23,4.1973706446812615E-5)\nRating(221,15,0.6652438913135681)\nRating(221,73,0.5706605208377349)\nRating(221,119,0.546357844839839)\nRating(221,235,0.5177499315254879)\nRating(221,2,0.47083256029784026)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=167","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=168","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=169","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=170","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=171","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=172"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544173119417_827891546","id":"20181207-085839_1430244179","dateCreated":"2018-12-07T08:58:39+0000","dateStarted":"2018-12-07T08:58:42+0000","dateFinished":"2018-12-07T08:58:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30009"},{"text":"// 推薦某用戶top5產品\n\n\n\n\n\n// user 1 購買紀錄\nval productForUser = trainRDD.keyBy(_.user).lookup(7696)\n\n// user 評分最高的10項產品\nproductForUser.sortBy(-_.rating).take(10).map(table => (product_name(table.product), table.rating)).foreach(println)\n\n// 推薦所有用戶10項產品\nval ProductsForUsers = model.recommendProductsForUsers(10)\n//ProductsForUsers.collect().foreach(println)\n//ProductsForUsers.take(10).foreach(println)\n//println(ProductsForUsers.first())\n\n//val df = ProductsForUsers.map(Row.fromSeq(_)).toDF()\n\n////\n//model.recommendUsersForProduct(10)\n//model.recommendForAllItems(10) \n// recommendUsersForProducts\n","user":"anonymous","dateUpdated":"2018-12-07T08:58:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"topKRecs: Array[org.apache.spark.mllib.recommendation.Rating] = Array(Rating(7696,3,0.6431615481391486), Rating(7696,23,0.42330291392066693), Rating(7696,11,0.4167356792525686), Rating(7696,0,0.29964098648449683), Rating(7696,24,0.2952154940172296))\nRating(7696,3,0.6431615481391486)\nRating(7696,23,0.42330291392066693)\nRating(7696,11,0.4167356792525686)\nRating(7696,0,0.29964098648449683)\nRating(7696,24,0.2952154940172296)\nproductForUser: Seq[org.apache.spark.mllib.recommendation.Rating] = WrappedArray(Rating(7696,3,500.0), Rating(7696,21270,500.0), Rating(7696,23,500.0), Rating(7696,17452,500.0))\n(40A90090UK,500.0)\n(4Z90Q25510,500.0)\n(40A70045UK,500.0)\n(4Z50Q25509,500.0)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=101","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=102","http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=103"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826556_-1034620162","id":"20181206-021319_1902964286","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T05:18:06+0000","dateFinished":"2018-12-07T05:18:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30010"},{"text":"println(productForUser.mkString(\"\\n\"))","user":"anonymous","dateUpdated":"2018-12-07T05:00:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Rating(1,1208,1.0)\nRating(1,6271,1.0)\nRating(1,8661,1.0)\nRating(1,10637,1.0)\nRating(1,12222,1.0)\nRating(1,3030,1.0)\nRating(1,4780,1.0)\nRating(1,949,1.0)\nRating(1,2659,1.0)\nRating(1,7686,1.0)\nRating(1,701,1.0)\nRating(1,11629,1.0)\nRating(1,11752,1.0)\nRating(1,6842,1.0)\nRating(1,2519,1.0)\nRating(1,2639,1.0)\nRating(1,10169,1.0)\nRating(1,1116,1.0)\nRating(1,4556,1.0)\nRating(1,902,1.0)\nRating(1,2446,1.0)\nRating(1,643,1.0)\nRating(1,1404,1.0)\nRating(1,2551,1.0)\nRating(1,7168,1.0)\nRating(1,11338,1.0)\nRating(1,2073,1.0)\nRating(1,6551,1.0)\nRating(1,9675,1.0)\nRating(1,9318,1.0)\nRating(1,5045,1.0)\nRating(1,6556,1.0)\nRating(1,6806,1.0)\nRating(1,11439,1.0)\nRating(1,758,1.0)\nRating(1,2197,1.0)\nRating(1,6419,1.0)\nRating(1,7334,1.0)\nRating(1,2231,1.0)\nRating(1,10494,1.0)\nRating(1,10003,1.0)\nRating(1,491,1.0)\nRating(1,2408,1.0)\nRating(1,2698,1.0)\nRating(1,12101,1.0)\nRating(1,3059,1.0)\nRating(1,7120,1.0)\nRating(1,12267,1.0)\nRating(1,804,1.0)\nRating(1,6963,1.0)\nRating(1,7265,1.0)\nRating(1,12116,1.0)\nRating(1,1115,1.0)\nRating(1,7809,1.0)\nRating(1,9348,1.0)\nRating(1,8883,1.0)\nRating(1,1967,1.0)\nRating(1,3496,1.0)\nRating(1,5846,1.0)\nRating(1,12234,1.0)\nRating(1,6786,1.0)\nRating(1,1338,1.0)\nRating(1,7424,1.0)\nRating(1,154,1.0)\nRating(1,555,1.0)\nRating(1,6279,1.0)\nRating(1,181,1.0)\nRating(1,9185,1.0)\nRating(1,8072,1.0)\nRating(1,10898,1.0)\nRating(1,10745,1.0)\nRating(1,10889,1.0)\nRating(1,349,1.0)\nRating(1,10759,1.0)\nRating(1,4255,1.0)\nRating(1,6737,1.0)\nRating(1,8881,1.0)\nRating(1,10370,1.0)\nRating(1,4744,1.0)\nRating(1,7077,1.0)\nRating(1,1974,1.0)\nRating(1,3911,1.0)\nRating(1,6188,1.0)\nRating(1,8436,1.0)\nRating(1,6909,1.0)\nRating(1,7757,1.0)\nRating(1,7969,1.0)\nRating(1,1086,1.0)\nRating(1,2152,1.0)\nRating(1,10708,1.0)\nRating(1,1859,1.0)\nRating(1,3068,1.0)\nRating(1,8775,1.0)\nRating(1,4706,1.0)\nRating(1,7385,1.0)\nRating(1,715,1.0)\nRating(1,2268,1.0)\nRating(1,2472,1.0)\nRating(1,11807,1.0)\nRating(1,404,1.0)\nRating(1,1358,1.0)\nRating(1,3357,1.0)\nRating(1,500,1.0)\nRating(1,6643,1.0)\nRating(1,5850,1.0)\nRating(1,11318,1.0)\nRating(1,9615,1.0)\nRating(1,10293,1.0)\nRating(1,5798,1.0)\nRating(1,1537,1.0)\nRating(1,11700,1.0)\nRating(1,1250,1.0)\nRating(1,2380,1.0)\nRating(1,8688,1.0)\nRating(1,7126,1.0)\nRating(1,7920,1.0)\nRating(1,321,1.0)\nRating(1,7124,1.0)\nRating(1,12261,1.0)\nRating(1,3593,1.0)\nRating(1,537,1.0)\nRating(1,4256,1.0)\nRating(1,5284,1.0)\nRating(1,6444,1.0)\nRating(1,5882,1.0)\nRating(1,8208,1.0)\nRating(1,5680,1.0)\nRating(1,6478,1.0)\nRating(1,10207,1.0)\nRating(1,9911,1.0)\nRating(1,7094,1.0)\nRating(1,9415,1.0)\nRating(1,1440,1.0)\nRating(1,2785,1.0)\nRating(1,3053,1.0)\nRating(1,6246,1.0)\nRating(1,10149,1.0)\nRating(1,11788,1.0)\nRating(1,5881,1.0)\nRating(1,3819,1.0)\nRating(1,4384,1.0)\nRating(1,5821,1.0)\nRating(1,9576,1.0)\nRating(1,1691,1.0)\nRating(1,2466,1.0)\nRating(1,8692,1.0)\nRating(1,11017,1.0)\nRating(1,1929,1.0)\nRating(1,402,1.0)\nRating(1,1683,1.0)\nRating(1,8347,1.0)\nRating(1,10318,1.0)\nRating(1,4955,1.0)\nRating(1,10690,1.0)\nRating(1,1485,1.0)\nRating(1,6634,1.0)\nRating(1,6980,1.0)\nRating(1,3177,1.0)\nRating(1,5013,1.0)\nRating(1,5472,1.0)\nRating(1,8411,1.0)\nRating(1,9967,1.0)\nRating(1,3927,1.0)\nRating(1,8919,1.0)\nRating(1,320,1.0)\nRating(1,2404,1.0)\nRating(1,2228,1.0)\nRating(1,4513,1.0)\nRating(1,1051,1.0)\nRating(1,1681,1.0)\nRating(1,1046,1.0)\nRating(1,6197,1.0)\nRating(1,8190,1.0)\nRating(1,10254,1.0)\nRating(1,586,1.0)\nRating(1,2873,1.0)\nRating(1,4956,1.0)\nRating(1,12205,1.0)\nRating(1,9090,1.0)\nRating(1,7893,1.0)\nRating(1,737,1.0)\nRating(1,8652,1.0)\nRating(1,8726,1.0)\nRating(1,11208,1.0)\nRating(1,139,1.0)\nRating(1,6629,1.0)\nRating(1,4879,1.0)\nRating(1,10877,1.0)\nRating(1,1373,1.0)\nRating(1,8073,1.0)\nRating(1,11231,1.0)\nRating(1,160,1.0)\nRating(1,331,1.0)\nRating(1,2938,1.0)\nRating(1,8405,1.0)\nRating(1,11843,1.0)\nRating(1,12069,1.0)\nRating(1,4029,1.0)\nRating(1,819,1.0)\nRating(1,10106,1.0)\nRating(1,262,1.0)\nRating(1,1286,1.0)\nRating(1,8019,1.0)\nRating(1,1700,1.0)\nRating(1,2529,1.0)\nRating(1,5559,1.0)\nRating(1,4871,1.0)\nRating(1,8850,1.0)\nRating(1,11547,1.0)\nRating(1,830,1.0)\nRating(1,2589,1.0)\nRating(1,9174,1.0)\nRating(1,697,1.0)\nRating(1,5439,1.0)\nRating(1,7474,1.0)\nRating(1,10360,1.0)\nRating(1,11167,1.0)\nRating(1,1717,1.0)\nRating(1,6052,1.0)\nRating(1,1896,1.0)\nRating(1,2997,1.0)\nRating(1,10686,1.0)\nRating(1,5394,1.0)\nRating(1,8070,1.0)\nRating(1,11572,1.0)\nRating(1,1673,1.0)\nRating(1,11889,1.0)\nRating(1,5415,1.0)\nRating(1,8018,1.0)\nRating(1,1007,1.0)\nRating(1,1225,1.0)\nRating(1,7276,1.0)\nRating(1,5304,1.0)\nRating(1,10387,1.0)\nRating(1,10657,1.0)\nRating(1,11432,1.0)\nRating(1,385,1.0)\nRating(1,7526,1.0)\nRating(1,7974,1.0)\nRating(1,10838,1.0)\nRating(1,1175,1.0)\nRating(1,2097,1.0)\nRating(1,3798,1.0)\nRating(1,8865,1.0)\nRating(1,8741,1.0)\nRating(1,990,1.0)\nRating(1,5347,1.0)\nRating(1,1113,1.0)\nRating(1,3372,1.0)\nRating(1,11643,1.0)\nRating(1,261,1.0)\nRating(1,10399,1.0)\nRating(1,11217,1.0)\nRating(1,2691,1.0)\nRating(1,7549,1.0)\nRating(1,11466,1.0)\nRating(1,970,1.0)\nRating(1,1372,1.0)\nRating(1,3306,1.0)\nRating(1,1618,1.0)\nRating(1,2546,1.0)\nRating(1,8193,1.0)\nRating(1,11365,1.0)\nRating(1,6076,1.0)\nRating(1,11142,1.0)\nRating(1,4673,1.0)\nRating(1,8759,1.0)\nRating(1,12256,1.0)\nRating(1,5695,1.0)\nRating(1,895,1.0)\nRating(1,1096,1.0)\nRating(1,4588,1.0)\nRating(1,581,1.0)\nRating(1,5856,1.0)\nRating(1,2092,1.0)\nRating(1,2391,1.0)\nRating(1,698,1.0)\nRating(1,6667,1.0)\nRating(1,10329,1.0)\nRating(1,732,1.0)\nRating(1,11499,1.0)\nRating(1,9048,1.0)\nRating(1,4629,1.0)\nRating(1,2256,1.0)\nRating(1,3861,1.0)\nRating(1,4300,1.0)\nRating(1,6391,1.0)\nRating(1,10526,1.0)\nRating(1,4354,1.0)\nRating(1,7259,1.0)\nRating(1,9964,1.0)\nRating(1,6038,1.0)\nRating(1,10656,1.0)\nRating(1,1757,1.0)\nRating(1,8176,1.0)\nRating(1,4153,1.0)\nRating(1,11134,1.0)\nRating(1,11376,1.0)\nRating(1,2804,1.0)\nRating(1,12064,1.0)\nRating(1,9045,1.0)\nRating(1,1446,1.0)\nRating(1,1633,1.0)\nRating(1,1943,1.0)\nRating(1,6600,1.0)\nRating(1,7738,1.0)\nRating(1,10546,1.0)\nRating(1,3100,1.0)\nRating(1,166,1.0)\nRating(1,3061,1.0)\nRating(1,9837,1.0)\nRating(1,7508,1.0)\nRating(1,730,1.0)\nRating(1,1708,1.0)\nRating(1,2798,1.0)\nRating(1,9877,1.0)\nRating(1,5698,1.0)\nRating(1,10469,1.0)\nRating(1,852,1.0)\nRating(1,1285,1.0)\nRating(1,1739,1.0)\nRating(1,4841,1.0)\nRating(1,567,1.0)\nRating(1,11585,1.0)\nRating(1,2597,1.0)\nRating(1,1530,1.0)\nRating(1,1150,1.0)\nRating(1,1490,1.0)\nRating(1,7739,1.0)\nRating(1,11778,1.0)\nRating(1,767,1.0)\nRating(1,7553,1.0)\nRating(1,7970,1.0)\nRating(1,2271,1.0)\nRating(1,4857,1.0)\nRating(1,2517,1.0)\nRating(1,6802,1.0)\nRating(1,6996,1.0)\nRating(1,9596,1.0)\nRating(1,1039,1.0)\nRating(1,2980,1.0)\nRating(1,3565,1.0)\nRating(1,4985,1.0)\nRating(1,8172,1.0)\nRating(1,9010,1.0)\nRating(1,7310,1.0)\nRating(1,7872,1.0)\nRating(1,11300,1.0)\nRating(1,1296,1.0)\nRating(1,2874,1.0)\nRating(1,5185,1.0)\nRating(1,431,1.0)\nRating(1,4383,1.0)\nRating(1,6111,1.0)\nRating(1,10057,1.0)\nRating(1,12188,1.0)\nRating(1,5524,1.0)\nRating(1,861,1.0)\nRating(1,3318,1.0)\nRating(1,5451,1.0)\nRating(1,8252,1.0)\nRating(1,1129,1.0)\nRating(1,9344,1.0)\nRating(1,1195,1.0)\nRating(1,1422,1.0)\n"}]},"apps":[],"jobName":"paragraph_1544158812069_-1153573923","id":"20181207-050012_755096649","dateCreated":"2018-12-07T05:00:12+0000","dateStarted":"2018-12-07T05:00:32+0000","dateFinished":"2018-12-07T05:00:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30011"},{"text":"// 选定id为24208的產品\r\nval itemId = 24208\r\n// 获取该物品的隐因子向量\r\nval itemFactor = model.productFeatures.lookup(itemId).head\r\n// 将该向量转换为jblas矩阵类型\r\nval itemVector = new DoubleMatrix(itemFactor)","user":"anonymous","dateUpdated":"2018-12-07T05:26:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"itemId: Int = 24208\nitemFactor: Array[Double] = Array(2.0763931388501078E-5, 3.005236249009613E-5, -5.19914465257898E-5, 1.5260146756190807E-4, 7.648244354641065E-5, 6.041851156624034E-5, -5.4529591579921544E-5, -7.980949885677546E-5, 1.8413495854474604E-4, -1.2238856288604438E-4, 7.788082439219579E-5, 7.240183185786009E-5)\nitemVector: org.jblas.DoubleMatrix = [2.0763931388501078E-5; 3.005236249009613E-5; -5.19914465257898E-5; 1.5260146756190807E-4; 7.648244354641065E-5; 6.041851156624034E-5; -5.4529591579921544E-5; -7.980949885677546E-5; 1.8413495854474604E-4; -1.2238856288604438E-4; 7.788082439219579E-5; 7.240183185786009E-5]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=104"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544153163876_-1372398537","id":"20181207-032603_853035203","dateCreated":"2018-12-07T03:26:03+0000","dateStarted":"2018-12-07T05:25:10+0000","dateFinished":"2018-12-07T05:25:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30012"},{"text":"// 计算產品24208与其他產品的相似度\r\nval sims = model.productFeatures.map{ case (id, factor) => \r\n    val factorVector = new DoubleMatrix(factor)\r\n    val sim = cosineSimilarity(factorVector, itemVector)\r\n    (id, sim)\r\n}\r\n// 获取与电影24208最相似的5個產品\r\nval sortedSims = sims.top(5)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\r\n// 打印结果\r\nprintln(sortedSims.mkString(\"\\n\"))","user":"anonymous","dateUpdated":"2018-12-07T05:28:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sims: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[791] at map at <console>:88\nsortedSims: Array[(Int, Double)] = Array((24208,1.0), (8249,1.0), (9419,0.9999999999999997), (11230,0.9999999999999994), (6239,0.9514230989102397))\n(24208,1.0)\n(8249,1.0)\n(9419,0.9999999999999997)\n(11230,0.9999999999999994)\n(6239,0.9514230989102397)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=105"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544160395884_-724433092","id":"20181207-052635_1181495816","dateCreated":"2018-12-07T05:26:35+0000","dateStarted":"2018-12-07T05:26:58+0000","dateFinished":"2018-12-07T05:27:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30013"},{"text":"// 打印电影567的影片名\r\nprintln(product_name(24208))\r\n// 获取和电影567最相似的11部电影(含567自己)\r\nval sortedSims2 = sims.top(10)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\r\n// 再打印和电影567最相似的10部电影\r\nsortedSims2.slice(1, 11).map{ case (id, sim) => (product_name(id), sim) }.mkString(\"\\n\")","user":"anonymous","dateUpdated":"2018-12-07T05:30:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"20HQS4GU00\nsortedSims2: Array[(Int, Double)] = Array((8249,1.0), (24208,1.0), (9419,0.9999999999999997), (11230,0.9999999999999994), (6239,0.9514230989102397), (6925,0.9514222209595533), (3073,0.945069206342315), (6218,0.9434799927307317), (7585,0.9434799927307317), (5670,0.940415018910653))\nres330: String =\n(20HQS4GU00,1.0)\n(10MR0021IX,0.9999999999999997)\n(20HR002SIX,0.9999999999999994)\n(20KQS1CV00,0.9514230989102397)\n(20KTS10S00,0.9514222209595533)\n(10NM0025IX,0.945069206342315)\n(20LTS3A900,0.9434799927307317)\n(20L6S86U00,0.9434799927307317)\n(20KTS10R00,0.940415018910653)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=107"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544160569133_852982064","id":"20181207-052929_305775328","dateCreated":"2018-12-07T05:29:29+0000","dateStarted":"2018-12-07T05:30:38+0000","dateFinished":"2018-12-07T05:30:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30014"},{"text":"ProductsForUsers.map(table => (product_name(table.product), table.rating)).foreach(println)","user":"anonymous","dateUpdated":"2018-12-07T02:49:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"MapPartitionsRDD[661] at mapValues at <console>:75\n"}]},"apps":[],"jobName":"paragraph_1544150706548_-1087505373","id":"20181207-024506_755598606","dateCreated":"2018-12-07T02:45:06+0000","dateStarted":"2018-12-07T02:45:09+0000","dateFinished":"2018-12-07T02:45:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30015"},{"text":"//MAP evaluation\n/* Compute recommendations for all users */\nval itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect()\nval itemMatrix = new DoubleMatrix(itemFactors)\nprintln(itemMatrix.rows, itemMatrix.columns)\n","user":"anonymous","dateUpdated":"2018-12-07T06:47:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"itemFactors: Array[Array[Double]] = Array(Array(0.01026960276067257, -0.4746765196323395, -0.05678558722138405, -0.033754173666238785, 0.21270030736923218, 0.2455088049173355, 0.3098829686641693, 0.0991937592625618, -0.16950370371341705, 0.01789691671729088, -0.2586459219455719, 0.14778737723827362), Array(0.02894260175526142, -0.022552132606506348, 0.11445462703704834, -0.015370170585811138, 0.030125999823212624, -0.012784705497324467, 0.13804937899112701, -0.08350232243537903, 0.020371969789266586, -0.005294233560562134, -0.05192679166793823, 0.026053179055452347), Array(-0.019226964563131332, -0.01767345890402794, -0.05895422399044037, 0.020856620743870735, 0.010196109302341938, -0.038020260632038116, 0.06418273597955704, 0.0029479111544787884, -0.014139467850327492, -0.0302533786743...itemMatrix: org.jblas.DoubleMatrix = [0.01026960276067257, -0.4746765196323395, -0.05678558722138405, -0.033754173666238785, 0.21270030736923218, 0.2455088049173355, 0.3098829686641693, 0.0991937592625618, -0.16950370371341705, 0.01789691671729088, -0.2586459219455719, 0.14778737723827362; 0.02894260175526142, -0.022552132606506348, 0.11445462703704834, -0.015370170585811138, 0.030125999823212624, -0.012784705497324467, 0.13804937899112701, -0.08350232243537903, 0.020371969789266586, -0.005294233560562134, -0.05192679166793823, 0.026053179055452347; -0.019226964563131332, -0.01767345890402794, -0.05895422399044037, 0.020856620743870735, 0.010196109302341938, -0.038020260632038116, 0.06418273597955704, 0.0029479111544787884, -0.014139467850327492, -0.030253378674387932, -0.12717337906360...(24209,12)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=160"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544161056340_33834624","id":"20181207-053736_887108495","dateCreated":"2018-12-07T05:37:36+0000","dateStarted":"2018-12-07T06:47:58+0000","dateFinished":"2018-12-07T06:48:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30016"},{"text":"val imBroadcast = sc.broadcast(itemMatrix)\nval allRecs = model.userFeatures.map{ case (userId, array) => \n  val userVector = new DoubleMatrix(array)\n  val scores = imBroadcast.value.mmul(userVector)\n  val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)\n  val recommendedIds = sortedWithId.map(_._2 + 1).toSeq\n  (userId, recommendedIds)\n}\n","user":"anonymous","dateUpdated":"2018-12-07T06:48:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"imBroadcast: org.apache.spark.broadcast.Broadcast[org.jblas.DoubleMatrix] = Broadcast(312)\nallRecs: org.apache.spark.rdd.RDD[(Int, Seq[Int])] = MapPartitionsRDD[1258] at map at <console>:103\n"}]},"apps":[],"jobName":"paragraph_1544161110383_71800444","id":"20181207-053830_562477089","dateCreated":"2018-12-07T05:38:30+0000","dateStarted":"2018-12-07T06:48:30+0000","dateFinished":"2018-12-07T06:48:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30017"},{"text":"val test = allRecs.take(3)\ntest.toDF().show()","user":"anonymous","dateUpdated":"2018-12-07T06:51:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"test: Array[(Int, Seq[Int])] = Array((0,WrappedArray(5818, 3416, 20581, 17199, 5576, 1, 15744, 978, 16466, 2679, 8246, 19373, 494, 2921, 5334, 1465, 8001, 18421, 15746, 22530, 5145, 9762, 736, 8003, 9455, 21554, 9215, 6793, 13810, 2924, 6552, 7046, 7520, 7280, 6801, 16963, 11, 11394, 5106, 20345, 20589, 16960, 13331, 23979, 9747, 18413, 14542, 8737, 18407, 8010, 1713, 6321, 10673, 6315, 23015, 8980, 16481, 2203, 10683, 19869, 5342, 15757, 8738, 8496, 13338, 7043, 264, 15276, 23020, 13335, 15025, 4867, 7529, 23262, 2209, 4871, 17449, 15026, 8494, 22287, 16487, 13572, 18914, 3894, 18655, 11880, 6075, 5835, 5111, 20114, 499, 1959, 12848, 4623, 23016, 7524, 9956, 12859, 18913, 4148, 1483, 13814, 10190, 22529, 19, 12616, 260, 3668, 3904, 16236, 21082, 21813, 4875, 14552, 758, 15523, 1237, 13...<console>:108: error: value toDF is not a member of Array[(Int, Seq[Int])]\n       test.toDF().show()\n            ^\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=163"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544165373852_-265766466","id":"20181207-064933_1264123519","dateCreated":"2018-12-07T06:49:33+0000","dateStarted":"2018-12-07T06:51:32+0000","dateFinished":"2018-12-07T06:51:33+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:30018"},{"text":"val userMovies = tableRDD.map{ case Rating(user, product, rating) => (user, product) }.groupBy(_._1)\nimport org.apache.spark.mllib.evaluation.RankingMetrics\nval predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n  val actual = actualWithIds.map(_._2)\n  (predicted.toArray, actual.toArray)\n}\nval rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)\nprintln(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision)\n","user":"anonymous","dateUpdated":"2018-12-07T06:48:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"userMovies: org.apache.spark.rdd.RDD[(Int, Iterable[(Int, Int)])] = ShuffledRDD[1261] at groupBy at <console>:78\nimport org.apache.spark.mllib.evaluation.RankingMetrics\npredictedAndTrueForRanking: org.apache.spark.rdd.RDD[(Array[Int], Array[Int])] = MapPartitionsRDD[1265] at map at <console>:108\nrankingMetrics: org.apache.spark.mllib.evaluation.RankingMetrics[Int] = org.apache.spark.mllib.evaluation.RankingMetrics@1620f146\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 4573.0 failed 4 times, most recent failure: Lost task 23.3 in stage 4573.0 (TID 41502, ip-172-31-3-65.ec2.internal, executor 180): ExecutorLostFailure (executor 180 exited caused by one of the running tasks) Reason: Container marked as failed: container_1544146771303_0001_01_000437 on host: ip-172-31-3-65.ec2.internal. Exit status: 50. Diagnostics: Exception from container-launch.\nContainer id: container_1544146771303_0001_01_000437\nExit code: 50\nStack trace: ExitCodeException exitCode=50:\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:972)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:869)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:235)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:299)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:83)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nContainer exited with a non-zero exit code 50\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n  at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\n  at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$stats$1.apply(DoubleRDDFunctions.scala:43)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:42)\n  at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply$mcD$sp(DoubleRDDFunctions.scala:48)\n  at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\n  at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$mean$1.apply(DoubleRDDFunctions.scala:48)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:47)\n  at org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecision$lzycompute(RankingMetrics.scala:108)\n  at org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecision(RankingMetrics.scala:87)\n  ... 60 elided\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=161"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544161145151_-1653731150","id":"20181207-053905_230491467","dateCreated":"2018-12-07T05:39:05+0000","dateStarted":"2018-12-07T06:48:36+0000","dateFinished":"2018-12-07T06:49:24+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:30019"},{"text":"// Evaluation\nval usersProducts = testRDD.map { case Rating(user,product,rating) => (user,product)}\nval predictions = model.predict(usersProducts).map{\n     case Rating(user, product, rating) => ((user, product), rating)\n }\n \nval ratingsAndPredictions = testRDD.map{\n  case Rating(user, product, rating) => ((user, product), rating)\n }.join(predictions)\n","user":"anonymous","dateUpdated":"2018-12-07T05:31:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"usersProducts: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[795] at map at <console>:64\npredictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[805] at map at <console>:84\nratingsAndPredictions: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[809] at join at <console>:88\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=108"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826557_1522594113","id":"20181206-034503_647953150","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T05:31:51+0000","dateFinished":"2018-12-07T05:32:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30020"},{"text":"import org.apache.spark.mllib.evaluation.RegressionMetrics\r\n // 创建预测评分-实际评分RDD\r\n val predictedAndTrue = ratingsAndPredictions.map { case ((user, product), (actual, predicted)) => (actual, predicted) }\r\n // 创建RegressionMetrics对象\r\n val regressionMetrics = new RegressionMetrics(predictedAndTrue)\r\n  \r\n // 打印MSE和RMSE\r\n println(\"Mean Squared Error = \" + regressionMetrics.meanSquaredError)\r\n println(\"Root Mean Squared Error = \" + regressionMetrics.rootMeanSquaredError)","user":"anonymous","dateUpdated":"2018-12-07T05:32:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.mllib.evaluation.RegressionMetrics\npredictedAndTrue: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[810] at map at <console>:90\nregressionMetrics: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@410815f9\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 3121.0 failed 4 times, most recent failure: Lost task 58.3 in stage 3121.0 (TID 31834, ip-172-31-3-65.ec2.internal, executor 155): ExecutorLostFailure (executor 155 exited caused by one of the running tasks) Reason: Container marked as failed: container_1544146771303_0001_01_000316 on host: ip-172-31-3-65.ec2.internal. Exit status: 50. Diagnostics: Exception from container-launch.\nContainer id: container_1544146771303_0001_01_000316\nExit code: 50\nStack trace: ExitCodeException exitCode=50:\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:972)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:869)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:235)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:299)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:83)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nContainer exited with a non-zero exit code 50\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n  at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n  at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n  at org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:57)\n  at org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:54)\n  at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n  at org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n  at org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\n  ... 60 elided\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-11-203.ec2.internal:4040/jobs/job?id=109"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1544146826557_1478111557","id":"20181206-052853_1019152204","dateCreated":"2018-12-07T01:40:26+0000","dateStarted":"2018-12-07T05:32:21+0000","dateFinished":"2018-12-07T05:33:06+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:30021"},{"text":".saveAsTextFile(somePath)\n// val UsersForProduct = model.recommendUsersForProduct(10)\n\n\n","user":"anonymous","dateUpdated":"2018-12-07T02:33:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"ProductsForUsers: org.apache.spark.rdd.RDD[(Int, Array[org.apache.spark.mllib.recommendation.Rating])] = MapPartitionsRDD[1409] at map at MatrixFactorizationModel.scala:218\n<console>:145: error: value recommendForUserSubset is not a member of org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n       val recommendForSubsetDF = model.recommendForUserSubset(Array(1,2), 10)\n                                        ^\n"}]},"apps":[],"jobName":"paragraph_1544146826558_662360406","id":"20181206-062956_380252158","dateCreated":"2018-12-07T01:40:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30022"},{"user":"anonymous","dateUpdated":"2018-12-07T01:40:26+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1544146826559_-485665782","id":"20181206-063543_1391271688","dateCreated":"2018-12-07T01:40:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30023"}],"name":"ETL","id":"2DYVNC4HZ","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}